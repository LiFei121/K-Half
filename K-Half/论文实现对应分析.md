# Temporal Fact Attention Module 论文实现对应分析

## 论文中的关键方程

### 1. 事实嵌入（Fact Embedding）
**论文公式 (1)**: `f(rx)_ij,ti = W1[ei || rx || ej || φ(d)]`
其中 `φ(d) = cos(dωt + bt)`，`d = tc - ti` 是更新时间间隔

### 2. 注意力分数（Attention Score）
**论文公式 (2)**: `α(rx)_ij = softmax(σ(W2 f(rx)_ij,ti))`
其中 `σ` 是参数化 Leaky ReLU 函数，`W2` 是可学习参数矩阵

### 3. 最终实体嵌入（Final Entity Embedding）
**论文公式 (3)**: `e'i = σ( (1/M) * Σ_{m=1 to M} Σ_{j∈Ni} Σ_{x∈Rij, t∈τ} α(rx),m_ij * f(rx),m_ij,ti )`
其中 `M` 是注意力头数，`τ` 是总时间戳数

### 4. 损失函数（Loss Function）
**论文公式 (4)**: `Lgat = Σ_{fij∈F} Σ_{f'ij∈F'} max(d'fij - dfij + θ, 0)`
其中 `dfij = ||ei + rx - ej||1` 是 L1 范数距离

---

## 代码实现对应关系

### ✅ 1. 时间嵌入函数 φ(d) = cos(dωt + bt)

**实现位置**: `K_Half.py` 第 84-86 行

```python
def get_time_embedding(self, t2):
    time_embedding = torch.cos(self.weight_t2 * t2 + self.bias_t2).repeat(self.num_ents, 1)
    return time_embedding
```

**对应关系**:
- `self.weight_t2` ↔ `ωt` (可学习时间单位)
- `self.bias_t2` ↔ `bt` (时间偏置)
- `t2` ↔ `d` (更新时间间隔)
- `torch.cos()` ↔ `cos()` 周期激活函数

**初始化位置**: `K_Half.py` 第 71-72 行
```python
self.weight_t2 = nn.Parameter(torch.randn(1, h_dim))
self.bias_t2 = nn.Parameter(torch.randn(1, h_dim))
```

---

### ⚠️ 2. 事实嵌入 f(rx)_ij,ti = W1[ei || rx || ej || φ(d)]

**部分实现位置**: 
- **时间部分**: `K_Half.py` 第 99-104 行（计算时间嵌入）
- **拼接部分**: `train.py` 第 180-192 行（生成事实嵌入）

```python
# K_Half.py 中计算时间嵌入
his_temp_embs = []
for i, fact in enumerate(batch_inputs):
    t2 = fact[3]  # 时间戳
    h_t = self.get_time_embedding(t2)
    his_temp_embs.append(h_t[batch_inputs[:, 0].long()])

# train.py 中拼接事实嵌入
def generate_fact_embeddings(facts, entity_emb, relation_dict, his_temp_embs=None):
    fact_embeddings = []
    for local_idx, (head, relation_id, tail, _) in enumerate(facts):
        entity_emb_head = entity_emb[head]
        entity_emb_tail = entity_emb[tail]
        relation_emb = relation_dict[relation_id]
        time_emb = his_temp_embs[local_idx][local_idx]
        
        fact_emb = torch.cat([entity_emb_head, entity_emb_tail, relation_emb, time_emb], dim=0)
        fact_embeddings.append(fact_emb)
    
    return torch.stack(fact_embeddings)
```

**对应关系**:
- `entity_emb_head` ↔ `ei` (头实体嵌入)
- `entity_emb_tail` ↔ `ej` (尾实体嵌入)
- `relation_emb` ↔ `rx` (关系嵌入)
- `time_emb` ↔ `φ(d)` (时间嵌入)
- `torch.cat([...])` ↔ `[ei || rx || ej || φ(d)]` (拼接操作)

**注意**: 
- ❌ 代码中没有显式的 `W1` 矩阵乘法来变换拼接后的特征
- ❌ 拼接顺序为 `[ei, ej, rx, φ(d)]`，与论文中的 `[ei || rx || ej || φ(d)]` 顺序不同

---

### ✅ 3. 注意力机制 α(rx)_ij = softmax(σ(W2 f(rx)_ij,ti))

**实现位置**: `layers.py` 第 72-110 行 (`SpGraphAttentionLayer.forward`)

```python
def forward(self, input, edge, edge_embed):
    N = input.size()[0]
    
    # 步骤1: 拼接特征 [ei || ej || rx]
    edge_h = torch.cat(
        (input[edge[0, :], :], input[edge[1, :], :], edge_embed[:, :]), dim=1).t()
    # edge_h: (2*in_dim + nrela_dim) x E
    
    # 步骤2: 计算 W2 * f (注意这里使用的是 W1，对应论文中的 W2)
    edge_m = self.a.mm(edge_h)  # self.a 对应 W2
    # edge_m: D * E
    
    # 步骤3: 计算 σ(W2 * f)
    powers = -self.leakyrelu(self.a_2.mm(edge_m).squeeze())
    edge_e = torch.exp(powers).unsqueeze(1)
    # edge_e: E (未归一化的注意力分数)
    
    # 步骤4: 计算 softmax (通过聚合和归一化实现)
    e_rowsum = self.special_spmm_final(edge, edge_e, N, edge_e.shape[0], 1)
    e_rowsum[e_rowsum == 0.0] = 1e-12
    
    edge_e = edge_e.squeeze(1)
    edge_e = self.dropout(edge_e)
    
    # 步骤5: 计算加权特征 α * f
    edge_w = (edge_e * edge_m).t()
    
    # 步骤6: 聚合到节点
    h_prime = self.special_spmm_final(edge, edge_w, N, edge_w.shape[0], self.out_features)
    h_prime = h_prime.div(e_rowsum)  # 归一化 (softmax)
    
    return F.elu(h_prime) if self.concat else h_prime
```

**对应关系**:
- `self.a` ↔ `W2` (可学习参数矩阵，第 62-64 行初始化)
- `self.a_2` ↔ 额外的注意力参数（用于计算注意力分数）
- `self.leakyrelu` ↔ `σ` (参数化 Leaky ReLU 函数)
- `edge_m` ↔ `W2 * f(rx)_ij,ti` (变换后的特征)
- `edge_e` ↔ `exp(σ(W2 * f))` (未归一化注意力分数)
- `h_prime.div(e_rowsum)` ↔ `softmax()` (归一化操作)
- `self.special_spmm_final` ↔ 稀疏矩阵乘法（用于高效聚合）

**注意**:
- ⚠️ 代码中在 `SpGraphAttentionLayer` 中拼接的是 `[ei, ej, rx]`，**没有包含时间嵌入 φ(d)**
- ⚠️ 时间嵌入是在 GAT 层之后才添加到事实嵌入中的（在 `generate_fact_embeddings` 函数中）
- 这可能导致时间信息没有直接参与注意力计算

---

### ✅ 4. 多注意力头机制 e'i = (1/M) * Σ_{m=1 to M} ...

**实现位置**: `K_Half.py` 第 20-26 行、第 44-45 行

```python
# 初始化多个注意力头
self.attentions = [SpGraphAttentionLayer(...) 
                   for _ in range(nheads)]

# 前向传播中拼接多个头的输出
x = torch.cat([att(x, edge_list, edge_embed)
               for att in self.attentions], dim=1)
```

**对应关系**:
- `self.attentions` ↔ 多个注意力头（M 个头）
- `torch.cat([...], dim=1)` ↔ `(1/M) * Σ` (拼接多个头的输出)
- `nheads` ↔ `M` (注意力头数)

---

### ⚠️ 5. 最终实体嵌入更新

**实现位置**: `K_Half.py` 第 111-114 行

```python
entities_upgraded = self.entity_embeddings.mm(self.W_entities)
out_entity_1 = entities_upgraded + mask.unsqueeze(-1).expand_as(out_entity_1) * out_entity_1
out_entity_1 = F.normalize(out_entity_1, p=2, dim=1)
```

**对应关系**:
- `out_entity_1` ↔ `e'i` (更新后的实体嵌入)
- `mask` ↔ 只更新出现在当前批次中的实体

**注意**:
- ❌ 代码中使用的是掩码机制，只更新特定实体，而不是按照论文公式 (3) 中的完整聚合
- ❌ 缺少论文中的 `σ` 激活函数（代码中使用的是 `F.elu` 和 `F.normalize`）

---

### ❌ 6. 损失函数 Lgat = Σ max(d'fij - dfij + θ, 0)

**论文中的损失函数**: 边际损失（Margin Loss）

**代码中的损失函数**: `train.py` 第 308 行
```python
loss = F.cross_entropy(logits[train_idx], labels[train_idx].long())
```

**对应关系**:
- ❌ **完全不匹配**: 代码使用的是交叉熵损失（用于分类任务），而不是论文中的边际损失
- ❌ 论文中的损失函数用于知识图谱嵌入任务（预测缺失的实体），而代码中的损失用于事实分类任务（长期/短期）

---

## 总结

### ✅ 已实现的部分:
1. **时间嵌入函数** `φ(d) = cos(dωt + bt)` - 完全实现
2. **注意力机制** `α(rx)_ij = softmax(σ(W2 f))` - 基本实现，但缺少时间信息
3. **多注意力头机制** - 完全实现
4. **事实嵌入拼接** - 部分实现，但顺序和时间整合方式不同

### ⚠️ 部分实现/有差异的部分:
1. **事实嵌入中的时间整合**: 时间嵌入没有直接参与注意力计算，而是在 GAT 之后才添加到事实嵌入中
2. **实体嵌入更新**: 使用了掩码机制而非完整的聚合公式
3. **激活函数**: 使用了 ELU 而非论文中提到的 `σ`

### ❌ 未实现的部分:
1. **损失函数**: 使用的是交叉熵损失而非论文中的边际损失
2. **事实嵌入中的 W1 变换**: 缺少显式的线性变换矩阵
3. **完整的时间聚合**: 论文中提到的时间戳聚合 `t∈τ` 在代码中处理方式不同

---

## 建议改进

1. **在注意力层中整合时间信息**: 将时间嵌入 `φ(d)` 直接添加到 `SpGraphAttentionLayer` 的特征拼接中
2. **调整事实嵌入顺序**: 将拼接顺序改为 `[ei || rx || ej || φ(d)]` 以匹配论文
3. **添加 W1 变换**: 在事实嵌入拼接后添加可学习的线性变换矩阵
4. **考虑实现边际损失**: 如果任务需要，可以实现论文中的边际损失函数


