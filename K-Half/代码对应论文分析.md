# Temporal Fact Attention Module ä»£ç å¯¹åº”è®ºæ–‡åˆ†æ

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ä»£ç ä¸­å“ªäº›éƒ¨åˆ†å®ç°äº†è®ºæ–‡"2.1 Temporal Fact Attention Module"ä¸­æåˆ°çš„æ–¹æ³•ã€‚

---

## ğŸ“‹ è®ºæ–‡æ ¸å¿ƒå…¬å¼

### å…¬å¼ (1): äº‹å®åµŒå…¥ï¼ˆFact Embeddingï¼‰
```
f(rx)_ij,ti = W1[ei || rx || ej || Ï†(d)]
```
å…¶ä¸­ `Ï†(d) = cos(dÏ‰t + bt)`ï¼Œ`d = tc - ti` æ˜¯æ›´æ–°æ—¶é—´é—´éš”

### å…¬å¼ (2): æ³¨æ„åŠ›åˆ†æ•°ï¼ˆAttention Scoreï¼‰
```
Î±(rx)_ij = softmax(Ïƒ(W2 f(rx)_ij,ti))
```
å…¶ä¸­ `Ïƒ` æ˜¯å‚æ•°åŒ– Leaky ReLU å‡½æ•°

### å…¬å¼ (3): æœ€ç»ˆå®ä½“åµŒå…¥ï¼ˆFinal Entity Embeddingï¼‰
```
e'i = Ïƒ( (1/M) * Î£_{m=1 to M} Î£_{jâˆˆNi} Î£_{xâˆˆRij, tâˆˆÏ„} Î±(rx),m_ij * f(rx),m_ij,ti )
```
å…¶ä¸­ `M` æ˜¯æ³¨æ„åŠ›å¤´æ•°

### å…¬å¼ (4): æŸå¤±å‡½æ•°ï¼ˆLoss Functionï¼‰
```
Lgat = Î£_{fijâˆˆF} Î£_{f'ijâˆˆF'} max(d'fij - dfij + Î¸, 0)
```
å…¶ä¸­ `dfij = ||ei + rx - ej||1` æ˜¯ L1 èŒƒæ•°è·ç¦»

---

## ğŸ” ä»£ç å®ç°å¯¹åº”å…³ç³»

### âœ… 1. æ—¶é—´åµŒå…¥å‡½æ•° Ï†(d) = cos(dÏ‰t + bt)

**ğŸ“ æ–‡ä»¶**: `K-Half/K_Half.py`

**ğŸ“ ä½ç½®**: ç¬¬ 71-72 è¡Œï¼ˆåˆå§‹åŒ–ï¼‰ã€ç¬¬ 84-86 è¡Œï¼ˆå®ç°ï¼‰

```python
# ç¬¬ 71-72 è¡Œï¼šåˆå§‹åŒ–æ—¶é—´åµŒå…¥å‚æ•°
self.weight_t2 = nn.Parameter(torch.randn(1, h_dim))  # å¯¹åº” Ï‰t
self.bias_t2 = nn.Parameter(torch.randn(1, h_dim))    # å¯¹åº” bt

# ç¬¬ 84-86 è¡Œï¼šå®ç°æ—¶é—´åµŒå…¥å‡½æ•°
def get_time_embedding(self, t2):
    time_embedding = torch.cos(self.weight_t2 * t2 + self.bias_t2).repeat(self.num_ents, 1)
    return time_embedding
```

**âœ… å¯¹åº”å…³ç³»**:
- `self.weight_t2` â†” `Ï‰t` (å¯å­¦ä¹ æ—¶é—´å•ä½)
- `self.bias_t2` â†” `bt` (æ—¶é—´åç½®)
- `t2` â†” `d` (æ›´æ–°æ—¶é—´é—´éš” `tc - ti`)
- `torch.cos()` â†” `cos()` å‘¨æœŸæ¿€æ´»å‡½æ•°

**âœ… çŠ¶æ€**: **å®Œå…¨å®ç°** âœ“

---

### âš ï¸ 2. äº‹å®åµŒå…¥ f(rx)_ij,ti = W1[ei || rx || ej || Ï†(d)]

**ğŸ“ æ–‡ä»¶**: 
- `K-Half/K_Half.py` ç¬¬ 99-104 è¡Œï¼ˆè®¡ç®—æ—¶é—´åµŒå…¥ï¼‰
- `K-Half/train.py` ç¬¬ 180-192 è¡Œï¼ˆç”Ÿæˆäº‹å®åµŒå…¥ï¼‰

```python
# K_Half.py ç¬¬ 99-104 è¡Œï¼šè®¡ç®—æ—¶é—´åµŒå…¥
his_temp_embs = []
for i, fact in enumerate(batch_inputs):
    t2 = fact[3]  # æ—¶é—´æˆ³
    h_t = self.get_time_embedding(t2)
    his_temp_embs.append(h_t[batch_inputs[:, 0].long()])

# train.py ç¬¬ 180-192 è¡Œï¼šæ‹¼æ¥äº‹å®åµŒå…¥
def generate_fact_embeddings(facts, entity_emb, relation_dict, his_temp_embs=None):
    fact_embeddings = []
    for local_idx, (head, relation_id, tail, _) in enumerate(facts):
        entity_emb_head = entity_emb[head]      # ei
        entity_emb_tail = entity_emb[tail]      # ej
        relation_emb = relation_dict[relation_id]  # rx
        time_emb = his_temp_embs[local_idx][local_idx]  # Ï†(d)
        
        # æ‹¼æ¥ï¼šæ³¨æ„é¡ºåºæ˜¯ [ei, ej, rx, Ï†(d)]ï¼Œè€Œéè®ºæ–‡ä¸­çš„ [ei || rx || ej || Ï†(d)]
        fact_emb = torch.cat([entity_emb_head, entity_emb_tail, relation_emb, time_emb], dim=0)
        fact_embeddings.append(fact_emb)
    
    return torch.stack(fact_embeddings)
```

**âš ï¸ å¯¹åº”å…³ç³»**:
- âœ… `entity_emb_head` â†” `ei` (å¤´å®ä½“åµŒå…¥)
- âœ… `entity_emb_tail` â†” `ej` (å°¾å®ä½“åµŒå…¥)
- âœ… `relation_emb` â†” `rx` (å…³ç³»åµŒå…¥)
- âœ… `time_emb` â†” `Ï†(d)` (æ—¶é—´åµŒå…¥)
- âœ… `torch.cat([...])` â†” `[ei || rx || ej || Ï†(d)]` (æ‹¼æ¥æ“ä½œ)
- âŒ **ç¼ºå°‘ W1 çŸ©é˜µå˜æ¢**: ä»£ç ä¸­æ²¡æœ‰æ˜¾å¼çš„ `W1` çŸ©é˜µä¹˜æ³•
- âš ï¸ **æ‹¼æ¥é¡ºåºä¸åŒ**: ä»£ç ä¸­æ˜¯ `[ei, ej, rx, Ï†(d)]`ï¼Œè®ºæ–‡ä¸­æ˜¯ `[ei || rx || ej || Ï†(d)]`

**âš ï¸ çŠ¶æ€**: **éƒ¨åˆ†å®ç°** âš ï¸ (ç¼ºå°‘ W1 å˜æ¢ï¼Œæ‹¼æ¥é¡ºåºä¸åŒ)

---

### âœ… 3. æ³¨æ„åŠ›åˆ†æ•° Î±(rx)_ij = softmax(Ïƒ(W2 f(rx)_ij,ti))

**ğŸ“ æ–‡ä»¶**: `K-Half/layers.py`

**ğŸ“ ä½ç½®**: ç¬¬ 48-122 è¡Œ (`SpGraphAttentionLayer` ç±»)

```python
# ç¬¬ 62-64 è¡Œï¼šåˆå§‹åŒ–æ³¨æ„åŠ›å‚æ•°
self.a = nn.Parameter(torch.zeros(
    size=(out_features, 2 * in_features + nrela_dim)))  # å¯¹åº” W2
self.a_2 = nn.Parameter(torch.zeros(size=(1, out_features)))

# ç¬¬ 72-110 è¡Œï¼šå‰å‘ä¼ æ’­å®ç°æ³¨æ„åŠ›æœºåˆ¶
def forward(self, input, edge, edge_embed):
    N = input.size()[0]
    
    # æ­¥éª¤1: æ‹¼æ¥ç‰¹å¾ [ei || ej || rx]
    # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰åŒ…å«æ—¶é—´åµŒå…¥ Ï†(d)
    edge_h = torch.cat(
        (input[edge[0, :], :], input[edge[1, :], :], edge_embed[:, :]), dim=1).t()
    # edge_h: (2*in_dim + nrela_dim) x E
    
    # æ­¥éª¤2: è®¡ç®— W2 * f
    edge_m = self.a.mm(edge_h)  # self.a å¯¹åº” W2
    # edge_m: D * E
    
    # æ­¥éª¤3: è®¡ç®— Ïƒ(W2 * f)
    powers = -self.leakyrelu(self.a_2.mm(edge_m).squeeze())
    edge_e = torch.exp(powers).unsqueeze(1)
    # edge_e: E (æœªå½’ä¸€åŒ–çš„æ³¨æ„åŠ›åˆ†æ•°)
    
    # æ­¥éª¤4: è®¡ç®— softmax (é€šè¿‡èšåˆå’Œå½’ä¸€åŒ–å®ç°)
    e_rowsum = self.special_spmm_final(edge, edge_e, N, edge_e.shape[0], 1)
    e_rowsum[e_rowsum == 0.0] = 1e-12
    
    edge_e = edge_e.squeeze(1)
    edge_e = self.dropout(edge_e)
    
    # æ­¥éª¤5: è®¡ç®—åŠ æƒç‰¹å¾ Î± * f
    edge_w = (edge_e * edge_m).t()
    
    # æ­¥éª¤6: èšåˆåˆ°èŠ‚ç‚¹
    h_prime = self.special_spmm_final(edge, edge_w, N, edge_w.shape[0], self.out_features)
    h_prime = h_prime.div(e_rowsum)  # å½’ä¸€åŒ– (softmax)
    
    return F.elu(h_prime) if self.concat else h_prime
```

**âœ… å¯¹åº”å…³ç³»**:
- âœ… `self.a` â†” `W2` (å¯å­¦ä¹ å‚æ•°çŸ©é˜µ)
- âœ… `self.leakyrelu` â†” `Ïƒ` (å‚æ•°åŒ– Leaky ReLU å‡½æ•°)
- âœ… `edge_m` â†” `W2 * f(rx)_ij,ti` (å˜æ¢åçš„ç‰¹å¾)
- âœ… `edge_e` â†” `exp(Ïƒ(W2 * f))` (æœªå½’ä¸€åŒ–æ³¨æ„åŠ›åˆ†æ•°)
- âœ… `h_prime.div(e_rowsum)` â†” `softmax()` (å½’ä¸€åŒ–æ“ä½œ)
- âœ… `self.special_spmm_final` â†” ç¨€ç–çŸ©é˜µä¹˜æ³•ï¼ˆé«˜æ•ˆèšåˆï¼‰

**âš ï¸ æ³¨æ„**:
- âš ï¸ ä»£ç ä¸­åœ¨æ³¨æ„åŠ›å±‚æ‹¼æ¥çš„æ˜¯ `[ei, ej, rx]`ï¼Œ**æ²¡æœ‰åŒ…å«æ—¶é—´åµŒå…¥ Ï†(d)**
- âš ï¸ æ—¶é—´åµŒå…¥æ˜¯åœ¨ GAT å±‚ä¹‹åæ‰æ·»åŠ åˆ°äº‹å®åµŒå…¥ä¸­çš„
- âš ï¸ è¿™æ„å‘³ç€æ—¶é—´ä¿¡æ¯æ²¡æœ‰ç›´æ¥å‚ä¸æ³¨æ„åŠ›è®¡ç®—

**âš ï¸ çŠ¶æ€**: **åŸºæœ¬å®ç°** âš ï¸ (ä½†ç¼ºå°‘æ—¶é—´ä¿¡æ¯åœ¨æ³¨æ„åŠ›ä¸­çš„æ•´åˆ)

---

### âœ… 4. å¤šæ³¨æ„åŠ›å¤´æœºåˆ¶ e'i = (1/M) * Î£_{m=1 to M} ...

**ğŸ“ æ–‡ä»¶**: `K-Half/K_Half.py`

**ğŸ“ ä½ç½®**: ç¬¬ 20-26 è¡Œï¼ˆåˆå§‹åŒ–ï¼‰ã€ç¬¬ 44-45 è¡Œï¼ˆå‰å‘ä¼ æ’­ï¼‰

```python
# ç¬¬ 20-26 è¡Œï¼šåˆå§‹åŒ–å¤šä¸ªæ³¨æ„åŠ›å¤´
self.attentions = [SpGraphAttentionLayer(num_nodes, nfeat,
                                         nhid,
                                         relation_dim,
                                         dropout=dropout,
                                         alpha=alpha,
                                         concat=True)
                   for _ in range(nheads)]  # nheads å¯¹åº” M

# ç¬¬ 44-45 è¡Œï¼šå‰å‘ä¼ æ’­ä¸­æ‹¼æ¥å¤šä¸ªå¤´çš„è¾“å‡º
x = torch.cat([att(x, edge_list, edge_embed)
               for att in self.attentions], dim=1)
# è¿™ç›¸å½“äº (1/M) * Î£_{m=1 to M} ...
```

**âœ… å¯¹åº”å…³ç³»**:
- âœ… `self.attentions` â†” å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼ˆM ä¸ªå¤´ï¼‰
- âœ… `torch.cat([...], dim=1)` â†” `(1/M) * Î£` (æ‹¼æ¥å¤šä¸ªå¤´çš„è¾“å‡º)
- âœ… `nheads` â†” `M` (æ³¨æ„åŠ›å¤´æ•°)

**âœ… çŠ¶æ€**: **å®Œå…¨å®ç°** âœ“

---

### âš ï¸ 5. æœ€ç»ˆå®ä½“åµŒå…¥æ›´æ–° e'i

**ğŸ“ æ–‡ä»¶**: `K-Half/K_Half.py`

**ğŸ“ ä½ç½®**: ç¬¬ 111-114 è¡Œ

```python
# ç¬¬ 111-114 è¡Œï¼šæ›´æ–°å®ä½“åµŒå…¥
entities_upgraded = self.entity_embeddings.mm(self.W_entities)
out_entity_1 = entities_upgraded + mask.unsqueeze(-1).expand_as(out_entity_1) * out_entity_1
out_entity_1 = F.normalize(out_entity_1, p=2, dim=1)
```

**âš ï¸ å¯¹åº”å…³ç³»**:
- âœ… `out_entity_1` â†” `e'i` (æ›´æ–°åçš„å®ä½“åµŒå…¥)
- âš ï¸ `mask` â†” åªæ›´æ–°å‡ºç°åœ¨å½“å‰æ‰¹æ¬¡ä¸­çš„å®ä½“ï¼ˆä½¿ç”¨æ©ç æœºåˆ¶ï¼‰
- âš ï¸ **ç¼ºå°‘å®Œæ•´çš„èšåˆå…¬å¼**: ä»£ç ä¸­ä½¿ç”¨æ©ç æœºåˆ¶ï¼Œè€Œä¸æ˜¯æŒ‰ç…§è®ºæ–‡å…¬å¼ (3) ä¸­çš„å®Œæ•´èšåˆ
- âš ï¸ **æ¿€æ´»å‡½æ•°ä¸åŒ**: ä»£ç ä¸­ä½¿ç”¨ `F.normalize` å’Œ `F.elu`ï¼Œè€Œä¸æ˜¯è®ºæ–‡ä¸­çš„ `Ïƒ`

**âš ï¸ çŠ¶æ€**: **éƒ¨åˆ†å®ç°** âš ï¸ (ä½¿ç”¨æ©ç æœºåˆ¶è€Œéå®Œæ•´èšåˆ)

---

### âŒ 6. æŸå¤±å‡½æ•° Lgat = Î£ max(d'fij - dfij + Î¸, 0)

**ğŸ“ æ–‡ä»¶**: `K-Half/train.py`

**ğŸ“ ä½ç½®**: ç¬¬ 308 è¡Œ

```python
# ç¬¬ 308 è¡Œï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±
loss = F.cross_entropy(logits[train_idx], labels[train_idx].long())
```

**âŒ å¯¹åº”å…³ç³»**:
- âŒ **å®Œå…¨ä¸åŒ¹é…**: ä»£ç ä½¿ç”¨çš„æ˜¯äº¤å‰ç†µæŸå¤±ï¼ˆç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰ï¼Œè€Œä¸æ˜¯è®ºæ–‡ä¸­çš„è¾¹é™…æŸå¤±
- âŒ è®ºæ–‡ä¸­çš„æŸå¤±å‡½æ•°ç”¨äºçŸ¥è¯†å›¾è°±åµŒå…¥ä»»åŠ¡ï¼ˆé¢„æµ‹ç¼ºå¤±çš„å®ä½“ï¼‰ï¼Œè€Œä»£ç ä¸­çš„æŸå¤±ç”¨äºäº‹å®åˆ†ç±»ä»»åŠ¡ï¼ˆé•¿æœŸ/çŸ­æœŸï¼‰

**âŒ çŠ¶æ€**: **æœªå®ç°** âŒ (ä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°)

---

## ğŸ“Š å®ç°æ€»ç»“

| è®ºæ–‡å…¬å¼ | å®ç°çŠ¶æ€ | ä»£ç ä½ç½® | è¯´æ˜ |
|---------|---------|---------|------|
| **Ï†(d) = cos(dÏ‰t + bt)** | âœ… å®Œå…¨å®ç° | `K_Half.py:84-86` | æ—¶é—´åµŒå…¥å‡½æ•° |
| **f(rx)_ij,ti = W1[ei \|\| rx \|\| ej \|\| Ï†(d)]** | âš ï¸ éƒ¨åˆ†å®ç° | `K_Half.py:99-104`<br>`train.py:180-192` | ç¼ºå°‘ W1 å˜æ¢ï¼Œæ‹¼æ¥é¡ºåºä¸åŒ |
| **Î±(rx)_ij = softmax(Ïƒ(W2 f))** | âš ï¸ åŸºæœ¬å®ç° | `layers.py:72-110` | ç¼ºå°‘æ—¶é—´ä¿¡æ¯æ•´åˆ |
| **e'i = (1/M) * Î£ Î± * f** | âœ… å®Œå…¨å®ç° | `K_Half.py:20-26,44-45` | å¤šæ³¨æ„åŠ›å¤´æœºåˆ¶ |
| **å®ä½“åµŒå…¥æ›´æ–°** | âš ï¸ éƒ¨åˆ†å®ç° | `K_Half.py:111-114` | ä½¿ç”¨æ©ç æœºåˆ¶ |
| **Lgat = Î£ max(d'fij - dfij + Î¸, 0)** | âŒ æœªå®ç° | `train.py:308` | ä½¿ç”¨äº¤å‰ç†µæŸå¤± |

---

## ğŸ”§ å»ºè®®æ”¹è¿›

### 1. **åœ¨æ³¨æ„åŠ›å±‚ä¸­æ•´åˆæ—¶é—´ä¿¡æ¯** â­â­â­
   - å°†æ—¶é—´åµŒå…¥ `Ï†(d)` ç›´æ¥æ·»åŠ åˆ° `SpGraphAttentionLayer` çš„ç‰¹å¾æ‹¼æ¥ä¸­
   - ä¿®æ”¹ `layers.py` ç¬¬ 77-78 è¡Œï¼Œå°†æ—¶é—´åµŒå…¥åŒ…å«åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­

### 2. **è°ƒæ•´äº‹å®åµŒå…¥é¡ºåº** â­â­
   - å°†æ‹¼æ¥é¡ºåºæ”¹ä¸º `[ei || rx || ej || Ï†(d)]` ä»¥åŒ¹é…è®ºæ–‡
   - ä¿®æ”¹ `train.py` ç¬¬ 188 è¡Œ

### 3. **æ·»åŠ  W1 å˜æ¢** â­â­
   - åœ¨äº‹å®åµŒå…¥æ‹¼æ¥åæ·»åŠ å¯å­¦ä¹ çš„çº¿æ€§å˜æ¢çŸ©é˜µ
   - åœ¨ `train.py` çš„ `generate_fact_embeddings` å‡½æ•°ä¸­æ·»åŠ 

### 4. **è€ƒè™‘å®ç°è¾¹é™…æŸå¤±** â­
   - å¦‚æœä»»åŠ¡éœ€è¦ï¼Œå¯ä»¥å®ç°è®ºæ–‡ä¸­çš„è¾¹é™…æŸå¤±å‡½æ•°
   - ä½†éœ€è¦æ³¨æ„ï¼Œå½“å‰ä»»åŠ¡æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œå¯èƒ½éœ€è¦ä¿ç•™äº¤å‰ç†µæŸå¤±

---

## ğŸ“ å…³é”®å‘ç°

1. **æ—¶é—´åµŒå…¥å‡½æ•°** âœ“ å®Œå…¨æŒ‰ç…§è®ºæ–‡å®ç°
2. **æ³¨æ„åŠ›æœºåˆ¶** âš ï¸ åŸºæœ¬å®ç°ï¼Œä½†æ—¶é—´ä¿¡æ¯æ²¡æœ‰ç›´æ¥å‚ä¸æ³¨æ„åŠ›è®¡ç®—
3. **å¤šæ³¨æ„åŠ›å¤´** âœ“ å®Œå…¨å®ç°
4. **æŸå¤±å‡½æ•°** âŒ ä½¿ç”¨äº†ä¸åŒçš„æŸå¤±å‡½æ•°ï¼ˆåˆ†ç±»ä»»åŠ¡ vs åµŒå…¥ä»»åŠ¡ï¼‰

**æ€»ä½“è¯„ä»·**: ä»£ç å®ç°äº†è®ºæ–‡ä¸­çš„æ ¸å¿ƒæ€æƒ³ï¼ˆæ—¶é—´åµŒå…¥ + æ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œä½†åœ¨ç»†èŠ‚å®ç°ä¸Šæœ‰ä¸€äº›å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯æ—¶é—´ä¿¡æ¯åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ•´åˆæ–¹å¼ã€‚


